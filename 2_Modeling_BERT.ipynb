{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47710934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbdda882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import zipfile\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c631b77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit</th>\n",
       "      <th>user_id</th>\n",
       "      <th>bust size</th>\n",
       "      <th>item_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>rating</th>\n",
       "      <th>rented for</th>\n",
       "      <th>review_text</th>\n",
       "      <th>body type</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>category</th>\n",
       "      <th>height</th>\n",
       "      <th>size</th>\n",
       "      <th>age</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fit</td>\n",
       "      <td>420272</td>\n",
       "      <td>34d</td>\n",
       "      <td>2260466</td>\n",
       "      <td>137lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>vacation</td>\n",
       "      <td>An adorable romper! Belt and zipper were a lit...</td>\n",
       "      <td>hourglass</td>\n",
       "      <td>So many compliments!</td>\n",
       "      <td>romper</td>\n",
       "      <td>5' 8\"</td>\n",
       "      <td>14</td>\n",
       "      <td>28.0</td>\n",
       "      <td>April 20, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fit</td>\n",
       "      <td>273551</td>\n",
       "      <td>34b</td>\n",
       "      <td>153475</td>\n",
       "      <td>132lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>other</td>\n",
       "      <td>I rented this dress for a photo shoot. The the...</td>\n",
       "      <td>straight &amp; narrow</td>\n",
       "      <td>I felt so glamourous!!!</td>\n",
       "      <td>gown</td>\n",
       "      <td>5' 6\"</td>\n",
       "      <td>12</td>\n",
       "      <td>36.0</td>\n",
       "      <td>June 18, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fit</td>\n",
       "      <td>360448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1063761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>party</td>\n",
       "      <td>This hugged in all the right places! It was a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It was a great time to celebrate the (almost) ...</td>\n",
       "      <td>sheath</td>\n",
       "      <td>5' 4\"</td>\n",
       "      <td>4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>December 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fit</td>\n",
       "      <td>909926</td>\n",
       "      <td>34c</td>\n",
       "      <td>126335</td>\n",
       "      <td>135lbs</td>\n",
       "      <td>8.0</td>\n",
       "      <td>formal affair</td>\n",
       "      <td>I rented this for my company's black tie award...</td>\n",
       "      <td>pear</td>\n",
       "      <td>Dress arrived on time and in perfect condition.</td>\n",
       "      <td>dress</td>\n",
       "      <td>5' 5\"</td>\n",
       "      <td>8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>February 12, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fit</td>\n",
       "      <td>151944</td>\n",
       "      <td>34b</td>\n",
       "      <td>616682</td>\n",
       "      <td>145lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>wedding</td>\n",
       "      <td>I have always been petite in my upper body and...</td>\n",
       "      <td>athletic</td>\n",
       "      <td>Was in love with this dress !!!</td>\n",
       "      <td>gown</td>\n",
       "      <td>5' 9\"</td>\n",
       "      <td>12</td>\n",
       "      <td>27.0</td>\n",
       "      <td>September 26, 2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit  user_id bust size  item_id  weight  rating     rented for  \\\n",
       "0  fit   420272       34d  2260466  137lbs    10.0       vacation   \n",
       "1  fit   273551       34b   153475  132lbs    10.0          other   \n",
       "2  fit   360448       NaN  1063761     NaN    10.0          party   \n",
       "3  fit   909926       34c   126335  135lbs     8.0  formal affair   \n",
       "4  fit   151944       34b   616682  145lbs    10.0        wedding   \n",
       "\n",
       "                                         review_text          body type  \\\n",
       "0  An adorable romper! Belt and zipper were a lit...          hourglass   \n",
       "1  I rented this dress for a photo shoot. The the...  straight & narrow   \n",
       "2  This hugged in all the right places! It was a ...                NaN   \n",
       "3  I rented this for my company's black tie award...               pear   \n",
       "4  I have always been petite in my upper body and...           athletic   \n",
       "\n",
       "                                      review_summary category height  size  \\\n",
       "0                               So many compliments!   romper  5' 8\"    14   \n",
       "1                            I felt so glamourous!!!     gown  5' 6\"    12   \n",
       "2  It was a great time to celebrate the (almost) ...   sheath  5' 4\"     4   \n",
       "3   Dress arrived on time and in perfect condition.     dress  5' 5\"     8   \n",
       "4                    Was in love with this dress !!!     gown  5' 9\"    12   \n",
       "\n",
       "     age         review_date  \n",
       "0   28.0      April 20, 2016  \n",
       "1   36.0       June 18, 2013  \n",
       "2  116.0   December 14, 2015  \n",
       "3   34.0   February 12, 2014  \n",
       "4   27.0  September 26, 2016  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_rtr = pd.read_json(\"data.zip/renttherunway_final_data.json\", lines=True)\n",
    "# pd.read_csv('data.zip', compression='zip')\n",
    "\n",
    "# Open the zip file\n",
    "with zipfile.ZipFile('data.zip') as z:\n",
    "    # Open the JSON file\n",
    "    with z.open('data/renttherunway_final_data.json') as f:\n",
    "        df_rtr = pd.read_json(f, lines=True)\n",
    "\n",
    "df_rtr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fd8f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df_rtr = df_rtr.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b04252f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rtr.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f580e21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit                   0\n",
       "user_id               0\n",
       "bust size         18392\n",
       "item_id               0\n",
       "weight            29955\n",
       "rating               81\n",
       "rented for           10\n",
       "review_text           0\n",
       "body type         14625\n",
       "review_summary        0\n",
       "category              0\n",
       "height              675\n",
       "size                  0\n",
       "age                 960\n",
       "review_date           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rtr.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca02a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit                0.00\n",
       "user_id            0.00\n",
       "bust size          9.56\n",
       "item_id            0.00\n",
       "weight            15.57\n",
       "rating             0.04\n",
       "rented for         0.01\n",
       "review_text        0.00\n",
       "body type          7.60\n",
       "review_summary     0.00\n",
       "category           0.00\n",
       "height             0.35\n",
       "size               0.00\n",
       "age                0.50\n",
       "review_date        0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the percentage of missing values in each column\n",
    "(df_rtr.isnull().sum() * 100 / len(df_rtr)).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f545e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d55b9cc2",
   "metadata": {},
   "source": [
    "### NLP Classifier Using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffdfe09",
   "metadata": {},
   "source": [
    "#### Build a simple machine learning model (like Logistic Regression or SVM) to classify fit.\n",
    "This is a baseline approach where text is tokenized, converted into a numerical feature matrix, and fed into a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c305dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine review_summary and review_text\n",
    "df_rtr['combined_text'] = df_rtr['review_summary'] + \" \" + df_rtr['review_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faf54c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Target Variable: Convert the 'fit' categories into numerical labels\n",
    "\n",
    "custom_order = np.array(['small', 'fit', 'large'])\n",
    "\n",
    "# Create LabelEncoder and set classes_\n",
    "le = LabelEncoder()\n",
    "le.classes_ = custom_order\n",
    "\n",
    "# Transform target variable\n",
    "df_rtr['fit_encoded'] = le.transform(df_rtr['fit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59cf754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF-IDF with stop words removed\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "X = vectorizer.fit_transform(df_rtr['combined_text'])\n",
    "y = df_rtr['fit_encoded']  # Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef23f61",
   "metadata": {},
   "source": [
    "#####\n",
    "X is the matrix created after transforming review_text (or combined text, including review_summary) into numerical form using techniques like TF-IDF Vectorizer. \n",
    "\n",
    "For instance, with TfidfVectorizer, X will be a sparse matrix where rows are reviews and columns are the top 5,000 most important words (based on TF-IDF scores).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ccbd6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1045)\t0.2033926255302943\n",
      "  (0, 250)\t0.26817707799101004\n",
      "  (0, 3656)\t0.2639702230665475\n",
      "  (0, 524)\t0.24374466411047713\n",
      "  (0, 4993)\t0.19984675536677607\n",
      "  (0, 2565)\t0.11346860992392349\n",
      "  (0, 2088)\t0.22597263015191554\n",
      "  (0, 2867)\t0.45424084080457444\n",
      "  (0, 1250)\t0.18160078836542434\n",
      "  (0, 4839)\t0.10461768100123445\n",
      "  (0, 494)\t0.31092890408024376\n",
      "  (0, 4732)\t0.2382951339460545\n",
      "  (0, 1608)\t0.22301854855442604\n",
      "  (0, 4904)\t0.19439727282251085\n",
      "  (0, 3250)\t0.19186631731094558\n",
      "  (0, 191)\t0.17819060605480072\n",
      "  (0, 3148)\t0.10221864303060015\n",
      "  (0, 2000)\t0.1259896593131114\n",
      "  (0, 2761)\t0.26001784396666633\n",
      "  (1, 1700)\t0.13960742542494722\n",
      "  (1, 1968)\t0.4041693646230863\n",
      "  (1, 3547)\t0.16086778757190046\n",
      "  (1, 1435)\t0.12054548780817384\n",
      "  (1, 3175)\t0.23752336203628668\n",
      "  (1, 3890)\t0.32499001822132983\n",
      "  :\t:\n",
      "  (192354, 3492)\t0.08735222503519205\n",
      "  (192354, 1738)\t0.04937602893230258\n",
      "  (192354, 4924)\t0.12013544041329292\n",
      "  (192354, 301)\t0.09862422272162094\n",
      "  (192354, 2908)\t0.07573449463295553\n",
      "  (192354, 4849)\t0.1544544809586157\n",
      "  (192354, 2003)\t0.12976212604014484\n",
      "  (192354, 3332)\t0.0911080266620032\n",
      "  (192354, 3020)\t0.15765375325112133\n",
      "  (192354, 4289)\t0.11129943432394461\n",
      "  (192354, 405)\t0.19790887366654591\n",
      "  (192354, 2087)\t0.12939036323912867\n",
      "  (192354, 447)\t0.11395566012202094\n",
      "  (192354, 4914)\t0.14256324277283053\n",
      "  (192354, 512)\t0.18287998127162955\n",
      "  (192354, 4234)\t0.16300620417239603\n",
      "  (192354, 4244)\t0.19836351800514\n",
      "  (192354, 3008)\t0.3389285945698721\n",
      "  (192354, 498)\t0.22317431490592063\n",
      "  (192354, 3220)\t0.18349227648315233\n",
      "  (192354, 913)\t0.20100970986461122\n",
      "  (192354, 4477)\t0.1668716456567983\n",
      "  (192354, 942)\t0.20059184829865903\n",
      "  (192354, 640)\t0.22375106272339723\n",
      "  (192354, 4193)\t0.24772167210729856\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a70557f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b746b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3feb5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine review_summary and review_text\n",
    "df_rtr['combined_text'] = df_rtr['review_summary'] + \" \" + df_rtr['review_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b1454ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target variable\n",
    "custom_order = np.array(['small', 'fit', 'large'])\n",
    "le = LabelEncoder()\n",
    "le.classes_ = custom_order\n",
    "df_rtr['fit_encoded'] = le.transform(df_rtr['fit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b46e7c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X = df_rtr['combined_text']\n",
    "y = df_rtr['fit_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0744d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b480f418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define pipeline\n",
    "# pipeline = Pipeline([\n",
    "#     ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "#     ('clf', LogisticRegression(max_iter=1000))\n",
    "# ])\n",
    "\n",
    "# # Define grid search parameters\n",
    "# param_grid = {\n",
    "#     'tfidf__max_features': [3000, 5000, 10000],\n",
    "#     'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "#     'clf__C': [0.1, 1, 10],\n",
    "# }\n",
    "\n",
    "# # Perform grid search\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Best model evaluation\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "# best_model = grid_search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8611398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9bf5f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d854cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pipeline with a Placeholder: Use a placeholder for the classifier step, such as 'clf'.\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', LogisticRegression())  # Placeholder; will be replaced by other classifiers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "336eb33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the Parameter Grid: Include different classifiers and their respective hyperparameters in the parameter grid.\n",
    "# param_grid = [\n",
    "#     {\n",
    "#         'tfidf__max_features': [3000, 5000],\n",
    "#         'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "#         'clf': [LogisticRegression(max_iter=1000)],\n",
    "#         'clf__C': [0.1, 1, 10]  # Logistic Regression hyperparameters\n",
    "#     },\n",
    "# #     {\n",
    "# #         'tfidf__max_features': [3000, 5000],\n",
    "# #         'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "# #         'clf': [RandomForestClassifier()],\n",
    "# #         'clf__n_estimators': [100, 200],  # Random Forest hyperparameters\n",
    "# #         'clf__max_depth': [None, 10]\n",
    "# #     },\n",
    "# #     {\n",
    "# #         'tfidf__max_features': [3000, 5000],\n",
    "# #         'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "# #         'clf': [SVC()],\n",
    "# #         'clf__C': [0.1, 1, 10],  # SVM hyperparameters\n",
    "# #         'clf__kernel': ['linear', 'rbf']\n",
    "# #     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910da202",
   "metadata": {},
   "source": [
    "#####\n",
    "When dealing with classification tasks, especially those with imbalanced classes, the F1-score is often a better metric than accuracy. The F1-score is the harmonic mean of precision and recall, and it provides a more balanced view of the model's performance.\n",
    "\n",
    "**f1_weighted** calculates the F1-score for each class and weights it by the number of true instances in each class. This ensures that all classes are fairly represented in the final score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a26e1fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run Grid Search: Use GridSearchCV to find the best combination of vectorizer settings, classifier, and hyperparameters.\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "# grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118fcee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a1f9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the Best Model: After training, evaluate the best model on the test set.\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# best_model = grid_search.best_estimator_\n",
    "# y_pred = best_model.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2cc036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11f3696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb0b242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Parameter Grid: Include different classifiers and their respective hyperparameters in the parameter grid.\n",
    "param_grid = [\n",
    "    {\n",
    "        'tfidf__max_features': [3000, 5000],\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "        'clf': [LogisticRegression(max_iter=1000)],\n",
    "        'clf__C': [0.1, 1, 10]  # Logistic Regression hyperparameters\n",
    "    },\n",
    "    {\n",
    "        'tfidf__max_features': [3000, 5000],\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "        'clf': [MultinomialNB()],\n",
    "        'clf__alpha': [0.01, 0.1, 1, 10]  # Smoothing parameter for Naive Bayes\n",
    "    },\n",
    "     {\n",
    "         'tfidf__max_features': [3000, 5000],\n",
    "         'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "         'clf': [SVC()],\n",
    "         'clf__C': [0.1, 1, 10],  # SVM hyperparameters\n",
    "         'clf__kernel': ['linear', 'rbf']\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Grid Search: Use GridSearchCV to find the best combination of vectorizer settings, classifier, and hyperparameters.\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f339449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Best Model: After training, evaluate the best model on the test set.\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1f0f53",
   "metadata": {},
   "source": [
    "### Word Embedding Models (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a6ef32",
   "metadata": {},
   "source": [
    "####\n",
    "Use pre-trained deep learning models, like BERT or RoBERTA, to get rich, contextualized word embeddings for the review_text.\n",
    "\n",
    "**What This Means:** \n",
    "Word embeddings represent the semantic meaning of text in a high-dimensional space.\n",
    "By averaging token embeddings, you get a single vector representing the entire review.\n",
    "\n",
    "**Why Use These?:** \n",
    "Embeddings capture more nuanced meanings compared to TF-IDF.\n",
    "Useful for complex tasks where semantic understanding matters.\n",
    "\n",
    "**Tools:** \n",
    "Use Hugging Face's transformers library to easily load and apply pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eacaed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e76b981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "inputs = tokenizer(df_rtr['combined_text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c2df8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787153d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88634bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1fd4ae",
   "metadata": {},
   "source": [
    "####\n",
    "Limit Input Sequence Length\n",
    "\n",
    "By default, models like BERT support a maximum sequence length of 512 tokens. Reducing this length (e.g., to 128) significantly lowers memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ea6b4",
   "metadata": {},
   "source": [
    "####\n",
    "Use a pre-trained model like BERT to extract embeddings for reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e568e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(df_rtr['combined_text'].tolist(), \n",
    "                   padding=True, \n",
    "                   truncation=True, \n",
    "                   max_length=128, \n",
    "                   return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you have combined_text in your DataFrame\n",
    "df_rtr['combined_text'] = df_rtr['review_summary'] + \" \" + df_rtr['review_text']\n",
    "\n",
    "# Extract the text data\n",
    "texts = df_rtr['combined_text'].tolist()  # Convert to a list for batch processing\n",
    "\n",
    "# Batch Processing - loops through texts in chunks of size batch_size to process them without overwhelming memory.\n",
    "batch_size = 8 # Start with a small batch size to avoid crashes\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i + batch_size]\n",
    "    inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "        np.save(f'batch_{i}.npy', batch_embeddings)\n",
    "\n",
    "# Load and concatenate embeddings\n",
    "embeddings = np.concatenate([np.load(f'batch_{i}.npy') for i in range(0, len(texts), batch_size)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d7bde",
   "metadata": {},
   "source": [
    "####\n",
    "Use these embeddings as features for classification or clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93475805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b7ef72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a600de93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f644df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccf337ad",
   "metadata": {},
   "source": [
    "#### Clustering and Visualization:\n",
    "\n",
    "Apply dimensionality reduction to embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa0a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, init='random')\n",
    "reduced_embeddings = tsne.fit_transform(X_train_vectorized)  # Use the same data as y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64377f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# TSNE with sparse input (using init=\"random\")\n",
    "tsne = TSNE(n_components=2, random_state=42, init=\"random\")\n",
    "reduced_embeddings = tsne.fit_transform(X_train_vectorized)\n",
    "\n",
    "# Visualize the TSNE output\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=y_train, cmap=\"viridis\", s=10, alpha=0.7)\n",
    "plt.colorbar(label=\"Fit Categories\")\n",
    "plt.xlabel(\"TSNE Component 1\")\n",
    "plt.ylabel(\"TSNE Component 2\")\n",
    "plt.title(\"TSNE Clusters Colored by Fit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac4fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6807d3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0b0376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da73d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4970548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd32d86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d77e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the reduced embeddings\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=10, alpha=0.7)\n",
    "plt.title(\"TSNE Visualization of Embeddings\")\n",
    "plt.xlabel(\"TSNE Component 1\")\n",
    "plt.ylabel(\"TSNE Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d43bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=y_train, cmap='viridis', s=10, alpha=0.7)\n",
    "plt.colorbar(label=\"Fit Categories\")  # Add a colorbar to indicate categories\n",
    "plt.title(\"TSNE Clusters Colored by Fit\")\n",
    "plt.xlabel(\"TSNE Component 1\")\n",
    "plt.ylabel(\"TSNE Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bac586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3ce628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6835c812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b94336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657cdaa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ed3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne = TSNE(n_components=2, random_state=42)\n",
    "# reduced_embeddings = tsne.fit_transform(embeddings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27fa089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform TSNE only on the training set\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_embeddings = tsne.fit_transform(X_train)  # Ensure X_train matches y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd35b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff28d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "013115f5",
   "metadata": {},
   "source": [
    "####\n",
    "Visualize clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c83d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=y_train, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f74ca",
   "metadata": {},
   "source": [
    "####  Combine with Other Features:\n",
    "\n",
    "Concatenate embeddings with features like age or manufacturer for richer analytics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c84d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = np.hstack([embeddings.numpy(), df_rtr[['age', 'rating']].values])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533f390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663b1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ca086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b559b815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d5800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796dfd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459ba53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a98585d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
