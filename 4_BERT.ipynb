{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e8268ea",
   "metadata": {},
   "source": [
    "### Word Embedding Models (BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5802b8",
   "metadata": {},
   "source": [
    "#### \n",
    "Use pre-trained deep learning models, like BERT or RoBERTA, to get rich, contextualized word embeddings for the review_text.\n",
    "\n",
    "**What This Means:** \n",
    "Word embeddings represent the semantic meaning of text in a high-dimensional space.\n",
    "By averaging token embeddings, you get a single vector representing the entire review.\n",
    "\n",
    "**Why Use These?:** \n",
    "Embeddings capture more nuanced meanings compared to TF-IDF.\n",
    "Useful for complex tasks where semantic understanding matters.\n",
    "\n",
    "**Tools:** \n",
    "Use Hugging Face's transformers library to easily load and apply pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4f2e46",
   "metadata": {},
   "source": [
    "####\n",
    "We also explored advanced techniques like embeddings and BERT. Embeddings convert words into dense numeric vectors, capturing semantic relationships and contextual meaning. BERT, in particular, is a state-of-the-art NLP model that processes text bidirectionally to understand the relationships between words.\n",
    "For example, BERT can distinguish between “tight shoulders” and “tight hips,” making it highly effective for analyzing fit-specific feedback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d1ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import zipfile\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f348a55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit</th>\n",
       "      <th>user_id</th>\n",
       "      <th>bust size</th>\n",
       "      <th>item_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>rating</th>\n",
       "      <th>rented for</th>\n",
       "      <th>review_text</th>\n",
       "      <th>body type</th>\n",
       "      <th>review_summary</th>\n",
       "      <th>category</th>\n",
       "      <th>height</th>\n",
       "      <th>size</th>\n",
       "      <th>age</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fit</td>\n",
       "      <td>420272</td>\n",
       "      <td>34d</td>\n",
       "      <td>2260466</td>\n",
       "      <td>137lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>vacation</td>\n",
       "      <td>An adorable romper! Belt and zipper were a lit...</td>\n",
       "      <td>hourglass</td>\n",
       "      <td>So many compliments!</td>\n",
       "      <td>romper</td>\n",
       "      <td>5' 8\"</td>\n",
       "      <td>14</td>\n",
       "      <td>28.0</td>\n",
       "      <td>April 20, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fit</td>\n",
       "      <td>273551</td>\n",
       "      <td>34b</td>\n",
       "      <td>153475</td>\n",
       "      <td>132lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>other</td>\n",
       "      <td>I rented this dress for a photo shoot. The the...</td>\n",
       "      <td>straight &amp; narrow</td>\n",
       "      <td>I felt so glamourous!!!</td>\n",
       "      <td>gown</td>\n",
       "      <td>5' 6\"</td>\n",
       "      <td>12</td>\n",
       "      <td>36.0</td>\n",
       "      <td>June 18, 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fit</td>\n",
       "      <td>360448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1063761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>party</td>\n",
       "      <td>This hugged in all the right places! It was a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>It was a great time to celebrate the (almost) ...</td>\n",
       "      <td>sheath</td>\n",
       "      <td>5' 4\"</td>\n",
       "      <td>4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>December 14, 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fit</td>\n",
       "      <td>909926</td>\n",
       "      <td>34c</td>\n",
       "      <td>126335</td>\n",
       "      <td>135lbs</td>\n",
       "      <td>8.0</td>\n",
       "      <td>formal affair</td>\n",
       "      <td>I rented this for my company's black tie award...</td>\n",
       "      <td>pear</td>\n",
       "      <td>Dress arrived on time and in perfect condition.</td>\n",
       "      <td>dress</td>\n",
       "      <td>5' 5\"</td>\n",
       "      <td>8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>February 12, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fit</td>\n",
       "      <td>151944</td>\n",
       "      <td>34b</td>\n",
       "      <td>616682</td>\n",
       "      <td>145lbs</td>\n",
       "      <td>10.0</td>\n",
       "      <td>wedding</td>\n",
       "      <td>I have always been petite in my upper body and...</td>\n",
       "      <td>athletic</td>\n",
       "      <td>Was in love with this dress !!!</td>\n",
       "      <td>gown</td>\n",
       "      <td>5' 9\"</td>\n",
       "      <td>12</td>\n",
       "      <td>27.0</td>\n",
       "      <td>September 26, 2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit  user_id bust size  item_id  weight  rating     rented for  \\\n",
       "0  fit   420272       34d  2260466  137lbs    10.0       vacation   \n",
       "1  fit   273551       34b   153475  132lbs    10.0          other   \n",
       "2  fit   360448       NaN  1063761     NaN    10.0          party   \n",
       "3  fit   909926       34c   126335  135lbs     8.0  formal affair   \n",
       "4  fit   151944       34b   616682  145lbs    10.0        wedding   \n",
       "\n",
       "                                         review_text          body type  \\\n",
       "0  An adorable romper! Belt and zipper were a lit...          hourglass   \n",
       "1  I rented this dress for a photo shoot. The the...  straight & narrow   \n",
       "2  This hugged in all the right places! It was a ...                NaN   \n",
       "3  I rented this for my company's black tie award...               pear   \n",
       "4  I have always been petite in my upper body and...           athletic   \n",
       "\n",
       "                                      review_summary category height  size  \\\n",
       "0                               So many compliments!   romper  5' 8\"    14   \n",
       "1                            I felt so glamourous!!!     gown  5' 6\"    12   \n",
       "2  It was a great time to celebrate the (almost) ...   sheath  5' 4\"     4   \n",
       "3   Dress arrived on time and in perfect condition.     dress  5' 5\"     8   \n",
       "4                    Was in love with this dress !!!     gown  5' 9\"    12   \n",
       "\n",
       "     age         review_date  \n",
       "0   28.0      April 20, 2016  \n",
       "1   36.0       June 18, 2013  \n",
       "2  116.0   December 14, 2015  \n",
       "3   34.0   February 12, 2014  \n",
       "4   27.0  September 26, 2016  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the zip file\n",
    "with zipfile.ZipFile('data.zip') as z:\n",
    "    # Open the JSON file\n",
    "    with z.open('data/renttherunway_final_data.json') as f:\n",
    "        df_rtr = pd.read_json(f, lines=True)\n",
    "\n",
    "df_rtr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f48fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine review_summary and review_text\n",
    "df_rtr['combined_text'] = df_rtr['review_summary'] + \" \" + df_rtr['review_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f09e53ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Target Variable: Convert the 'fit' categories into numerical labels\n",
    "\n",
    "custom_order = np.array(['small', 'fit', 'large'])\n",
    "\n",
    "# Create LabelEncoder and set classes_\n",
    "le = LabelEncoder()\n",
    "le.classes_ = custom_order\n",
    "\n",
    "# Transform target variable\n",
    "df_rtr['fit_encoded'] = le.transform(df_rtr['fit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dae8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X = df_rtr['combined_text']\n",
    "y = df_rtr['fit_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bbdf1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cea8e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and batch process\n",
    "texts = df_rtr['combined_text'].tolist()\n",
    "batch_size = 8  # Adjust based on memory availability\n",
    "all_embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f75cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process in batches\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i + batch_size]\n",
    "    inputs = tokenizer(batch_texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    inputs = {key: val.to('cuda' if torch.cuda.is_available() else 'cpu') for key, val in inputs.items()}  # Move to GPU if available\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()  # Average token embeddings\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "662d2e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all embeddings into a single array\n",
    "embeddings = np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2137a7c",
   "metadata": {},
   "source": [
    "####\n",
    "Use these embeddings as features for classification or clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfaf3aa",
   "metadata": {},
   "source": [
    "#### Clustering and Visualization:\n",
    "\n",
    "Apply dimensionality reduction to embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd167cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_embeddings = tsne.fit_transform(embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec82545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, init='random')\n",
    "reduced_embeddings = tsne.fit_transform(X_train_vectorized)  # Use the same data as y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a31ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# TSNE with sparse input (using init=\"random\")\n",
    "tsne = TSNE(n_components=2, random_state=42, init=\"random\")\n",
    "reduced_embeddings = tsne.fit_transform(X_train_vectorized)\n",
    "\n",
    "# Visualize the TSNE output\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=y_train, cmap=\"viridis\", s=10, alpha=0.7)\n",
    "plt.colorbar(label=\"Fit Categories\")\n",
    "plt.xlabel(\"TSNE Component 1\")\n",
    "plt.ylabel(\"TSNE Component 2\")\n",
    "plt.title(\"TSNE Clusters Colored by Fit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of the reduced embeddings\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], s=10, alpha=0.7)\n",
    "plt.title(\"TSNE Visualization of Embeddings\")\n",
    "plt.xlabel(\"TSNE Component 1\")\n",
    "plt.ylabel(\"TSNE Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616dc0e9",
   "metadata": {},
   "source": [
    "####\n",
    "Visualize clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef8720",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=y_train, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65496c5",
   "metadata": {},
   "source": [
    "####  Combine with Other Features:\n",
    "\n",
    "Concatenate embeddings with features like age or manufacturer for richer analytics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = np.hstack([embeddings.numpy(), df_rtr[['age', 'rating']].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ad0fcb",
   "metadata": {},
   "source": [
    "#### While powerful, BERT’s computational cost and complexity make it less practical for this relatively straightforward task. Simpler methods like TF-IDF paired with Logistic Regression proved more efficient and equally effective in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b800de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
